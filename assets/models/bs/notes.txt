Ran python export_broadcast_static.py checkpoints/full_character_model_standalone_hwsfuto8/full-model-99-0.734.ckpt exported_broadcast_prev --opset 17
  └ Export complete. Files written to: exported_broadcast_prev

• Explored
  └ List exported_broadcast_prev

─ Worked for 2m 08s ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

• Here’s what I did and how to proceed.

  What I exported

  - Created a broadcast-enabled, static-quantized Android export from your previous run’s best checkpoint:
      - Input: checkpoints/full_character_model_standalone_hwsfuto8/full-model-99-0.734.ckpt
      - Output folder: exported_broadcast_prev
      - Files:
          - exported_broadcast_prev/swipe_encoder_android.onnx
          - exported_broadcast_prev/swipe_decoder_android.onnx
          - exported_broadcast_prev/model_config.json
          - exported_broadcast_prev/tokenizer_config.json

  Android integration

  - Drop-in compatible: I/O signatures match your current app.
      - Encoder: trajectory_features (f32), nearest_keys (i32), actual_length (i32) → encoder_output (f32)
      - Decoder: memory (f32), target_tokens (i32), actual_src_length (i32) → log_probs (f32)
  - Broadcast built-in: If you pass memory with batch=1 and target_tokens with batch=num_beams, the decoder expands memory internally. If you already expand memory in the app,
    both ways work.

  Important constraint (keeps things fast and stable)

  - Fixed dec_seq=20 in this exported decoder (matches your Android limit).
      - Why: PyTorch’s Transformer creates some reshape constants at trace-time; using 20 avoids runtime shape issues and you’ve said 20 is fine.
      - Action: In the app, keep max decode length at 20 (as you’re doing).

  Performance notes

  - Static INT8 quantization (weights per-channel, activations UINT8) gives faster load and runtime than dynamic quant.
  - Broadcast decoder reduces work during beam search (one encoder pass per swipe; batched decoder pass across beams).
  - You’ll see benign ONNXRuntime quant warnings about LayerNorm not per-channel quantizable; that’s expected and fine.

  Training: your questions answered

  - CTC instability — how to detect:
      - Red flags: NaNs/infs, sudden CTC loss spikes, worse val word acc despite stable CE, or wrong_examples showing odd blanks/collapses.
      - What to do: lower AUX_CTC_WEIGHT to 0.15–0.2, or warm-in (0.0 for a few epochs → 0.3). Keep LABEL_SMOOTH=0.0 while using CTC.
  - Are params sensible for a 4090M 16 GB? Yes.
      - Mixed precision + GradScaler + OneCycle warmup are good defaults for this GPU.
      - That “scheduler.step() before optimizer.step()” warning popped once; harmless given our order (we call optimizer then scheduler each step). It can appear due to scheduler
        fast-forwarding.
      - If CPU has headroom, bump NUM_WORKERS to 8–16 to increase throughput; persistent_workers stays on.
  - Dropout, teacher forcing, scaler:
      - Dropout 0.25 is fine; if you want a touch more stability, 0.2 is also reasonable.
      - Teacher forcing schedule: I set SS_WARMUP=10, SS_END=0.85 with greedy rollout — good for repeats. If still underfitting repeats, raise SS_END to 0.9–0.95.
      - GradScaler + AMP are on and appropriate.
  - Beam search / double-letter accuracy:
      - Keep LENGTH_PENALTY_ALPHA=0.4–0.5 and beam size 5–8.
      - You already have a sampler boosting doubles; defaults are tuned up. If “hello/lasso” still weak, consider DOUBLE_MUL=3.0 and cap MAX_SAMPLE_WEIGHT=12.

  How to rerun export yourself

  - After you’re happy with a new checkpoint (in the new folder checkpoints/full_character_model_standalone_hwsfuto10), run:
      - python export_broadcast_static.py checkpoints/full_character_model_standalone_hwsfuto10/last.ckpt exported_broadcast --opset 17
  - Copy the two ONNX files and the JSON configs into your Android project. No code change required.

  Anything else you want me to tune? I can:

  - Log CTC loss per epoch if you want visibility.
  - Bump DataLoader workers for throughput.
  - Add a warm-up schedule for AUX_CTC_WEIGHT (e.g., ramp from 0→0.3 over N epochs).